% !TEX root = my-thesis.tex
%
\chapter{Related Work}
\label{ch:related-work}

In the following, we summarize related work. 
First, we present several knowledge graphs that focus on causal and commonsense knowledge. 
Afterward, we
provide an overview of approaches for causal question answering. These approaches mainly employ pattern-based searches, embeddings, and language models. 
To the best of our knowledge, we are the first to use reinforcement learning for causal question answering. Lastly, we discuss approaches that apply reinforcement learning to reasoning and 
question answering tasks on knowledge graphs.

\paragraph{Causal Knowledge Graphs}
ConceptNet~\cite{Speer2017ConceptNet} is a general knowledge graph consisting of 36 relations between natural language terms, 
including a \textit{Causes} relation. CauseNet~\cite{Heindorf2020Causenet} and Cause Effect Graph~\cite{Li2020CauseEffectGraph} 
specifically focus on causal relations extracted via linguistic patterns from web sources like Wikipedia and ClueWeb12.\footnote{\url{https://lemurproject.org/clueweb12/}}

In contrast, ATOMIC~\cite{Sap2019ATOMIC} focuses on inferential knowledge of commonsense reasoning in everyday life. Specifically, ATOMIC consists of \textit{``If-Event-Then-X''} relations based on social interactions or real-world events. 
ATOMIC$^{20}_{20}$~\cite{Hwang2021COMET} selects relations from ATOMIC and ConceptNet to create an improved graph while adding more relations via crowdsourcing. Instead, West et al.~\cite{West2021Symbolic} automate the curation of causal relations by clever prompting of a language model. Finally, CSKG~\cite{Ilievski2021CSKG} builds a consolidated graph combining seven knowledge graphs, including ConceptNet and ATOMIC.
For this thesis, we use CauseNet because most of the other knowledge graphs either focus on different relations types or are not limited to causal relations.
However, future work may involve combining multiple knowledge graphs, which we discuss further in Chapter~\ref{ch:discussion}.

\paragraph{Causal Question Answering}

Currently, only a few approaches directly focus on the causal question answering task. Most of these approaches focus on binary questions, i.e., questions such as ``\textit{Does X cause Y?}'' which expect a ``yes'' or ``no'' answer. 
Kayesh et al.~\cite{KayeshCausalTransfer2020} model the task as a transfer learning approach. Therefore, they extract cause-effect pairs from news articles via causal cue words. Subsequently, they transform the pairs into sentences of the form ``\textit{X may cause Y}'' and use them to finetune BERT~\cite{DevlinBert2019}. Similarly, Hassanzadesh et al. \cite{HassanzadeshCausalQA2019} employ large-scale text mining to answer binary causal questions. They introduce several unsupervised approaches ranging from simple string matching to embeddings computed via BERT. Each approach uses the text corpora to find evidence for a potential causal relation. Afterward, the evidence is used to compute a score that results in a ``yes'' or ``no'' answer depending on a threshold value.


Instead, Sharp et al.~\cite{SharpCausalQAEmbeddings2016} consider multiple-choice questions of the form ``\textit{What causes X?}''. First, they mine cause-effect pairs from Wikipedia via syntactic patterns and train an embedding model to capture the semantics between them. At inference time, they compute the embedding similarity between the question and each answer candidate. The approach by Dalal et al.~\cite{DalalCausalQAEnhancing2021, DalalCausalQAISWC2021}
combines a language model with CauseNet~\cite{Heindorf2020Causenet}.
Given a question, they apply string matching to extract relevant causal relations from CauseNet. Subsequently, they provide the question with the causal relations  as additional context to a language model and let the language model answer the question. 
%In our work, we consider a similar idea for complex questions by selecting relevant paths via a reinforcement learning agent.

Another approach that combines knowledge graphs and language models is QA-GNN~\cite{Yasunaga2021QAGNN}.
Specifically, QA-GNN creates an augmented graph by scoring entities and relations with a pre-trained language model according to their relevance to the question. Then, QA-GNN applies a graph attention network (GAT)~\cite{Velickovic2018GAT} on the augmented graph to answer the question. Likewise, we use their scoring function to apply the reinforcement learning agent on an augmented graph to provide a better reward signal, similar to the reward shaping strategies by prior reinforcement learning approaches on knowledge graphs~\cite{Lin2020RewardShaping, Qiu2020Stepwise}.


%\cite{Lu2019ComplexSteiner} Build a quasi KG from text documents which contain entities and phrases relevant to the question. Assign weights to nodes and edges. Find answers by solving a Group Steiner Tree problem~\cite{ReichSteiner1989} on the created quasi KG. 

\paragraph{Knowledge Graph Reasoning with Reinforcement Learning}

In recent years, reinforcement learning on knowledge graphs has been successfully applied in link prediction~\cite{Das2018Minerva}, fact-checking~\cite{Xiong2017DeePpath}, or question answering~\cite{Qiu2020Stepwise}. Given a source and a target entity, DeepPath~\cite{Xiong2017DeePpath} learns to find paths between them. In the first step, DeepPath is trained in a supervised manner on paths found by BFS. Afterward, DeepPath applies REINFORCE~\cite{Williams1992REINFORCE} policy gradients to improve the policy further.
During inference time, the paths are used to predict links between entities or check the validity of triples. Subsequently, MINERVA~\cite{Das2018Minerva} improves on DeepPath by introducing an LSTM~\cite{Hochreiter1997LSTM} into the policy network to account for the path history. Moreover, MINERVA does not require knowledge of the target entity and is trained end-to-end without supervision at the start. Lin et al.~\cite{Lin2020RewardShaping} propose two improvements for MINERVA. First, they apply reward shaping by scoring the paths with a pre-trained KG embedding model~\cite{Dettmers2018ConvE} to reduce the problem of sparse rewards. Second, they introduce a technique called action dropout, which randomly disables edges at each step. Action dropout serves as additional regularization and helps the agent learn diverse paths.


Contrary to these approaches, M-Walk~\cite{Shen2018MWalk} does not rely on REINFORCE, but instead looks at the problem from a model-based viewpoint. 
Like AlphaZero~\cite{Silver2018AlphaZero}, M-Walk applies Monte Carlo Tree Search (MCTS) as a policy improvement operator. Thus, at each step, M-Walk applies MCTS to produce trajectories of an improved policy and subsequently trains the current policy to imitate the improved one. GaussianPath~\cite{Wan2021GaussianPath} takes a Bayesian view of the problem and represents each entity by a gaussian distribution to better model uncertainty.

While all these approaches focus on link prediction or fact-checking, there has been some work on natural language question answering.
Qiu et al.~\cite{Qiu2020Stepwise} introduce the Stepwise Reasoning Network (SRN).
They observe that for questions requiring multiple hops, different parts of the question should be important at each time step. Therefore, they introduce an attention mechanism into the policy network to enable the agent to attend to different parts of the question at every step. In turn, this should help the agent to focus on the aspect of the question that is important for the current decision. Additionally, SRN utilizes extra reward signals by considering the semantic similarities between the question and the action history via a potential-based reward function~\cite{Ng1999Potential}. CONQUER~\cite{Kaiser2021Reinforcement} employs REINFORCE policy gradients for conversational question answering. Multiple agents search through the graph in parallel and aggregate their answers at the end. Importantly, the reward depends on user feedback. A positive reward corresponds to a new question and a negative reward to a reformulation of the last question.


%\begin{itemize}
%    \item Variational Reasoning for Question Answering with Knowledge Graph
%    \item Anytime Bottom-Up Rule Learning for Knowledge Graph Completion
%    \item Safran: https://arxiv.org/pdf/2109.08002.pdf
%\end{itemize}




%\textbf{https://github.com/THU-KEG/Knowledge\_Graph\_Reasoning\_Papers}