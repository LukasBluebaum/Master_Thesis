\begin{algorithm}
	\DontPrintSemicolon
	\caption{Pseudocode of the training procedure for the policy 
	network $\pi_{\theta}(a_t | s_t)$ and value network $V_\psi(s_t)$ 
	as described in Section~\ref{sec:approach-description}.}%
	\label{alg:algorithm}%
	\textbf{Input:} Knowledge graph $\mathcal{K}$, questions $questions$, 
	optimization steps $steps$, batch size $B$, path rollout length $T$, learning rate $lr$,
	entropy weight $\beta$, discount factor $\gamma$, GAE lambda $\lambda$  \\
	\textbf{Output:} Trained Agent $\theta$, $\psi$ \\
	\SetKwFunction{F}{TrainAgent}
	\SetKwProg{Fn}{Function}{:}{\KwRet $\theta$}
	\Fn{\F{$\mathcal{K}$, $questions$, $steps$, $B$, $T$, $lr$, $\beta$, $\gamma$, $\lambda$}}{%
		$processed\_questions = [ \ ]$\;
		\For{\textbf{each} $q$ \textbf{in} $questions$}{%
			Link the cause and effect of $q$ to entities $e_c$ and $e_e$ in $\mathcal{K}$\;
			Compute embeddings $\mathbf{q}$, $\mathbf{e_c}$ for $q$, $e_c$\;
			$processed\_questions.append((\mathbf{q}, e_c, \mathbf{e_c}, e_e))$
		}
		\;
		Initialize agent weights $\theta$, $\psi$\;
		$path\_rollouts = [\,]$\;
		$step = 0$\;
		\While{step < steps}{%
			$path\_rollout = [\,]$\;
			$(\mathbf{q}, e_c, \mathbf{e_c}, e_e) = SampleUniform(processed\_questions)$\;
			$s_0 = (\mathbf{q}, e_c, \mathbf{e_c}, \mathbf{0}, e_e)$\; \label{ln:17}
			\For{$t = 0$ \textbf{to} $T-1$}{%
				$a_t = SampleCategorical(\pi_{\theta}(a_t | s_t))$\;
				Receive state $s_{t+1} = \delta (s_t, a_t)$ and reward $r_t = \mathcal{R}(s_{t+1})$\; \label{ln:20}
				$path\_rollout.append((s_t, a_t, r_t))$\;
			}
			$path\_rollouts.append(path\_rollout)$\;
			\;
			\If{$|path\_rollouts| = B$}{%
				Compute the GAE $\mathcal{A}_t^{\psi}$ and $\lambda$-returns $R_t(\lambda)$ for each episode in $path\_rollouts$ using $\lambda, \gamma$ \;
				$policy\_update, \ value\_update = 0$\;
				\For{\textbf{each} $((s_0, a_0, r_0, \mathcal{A}_0^{\psi}, R_0(\lambda)),\dots, (s_{T-2}, a_{T-2}, r_{T-2}, \mathcal{A}_{T-2}^{\psi}, R_{T-2}(\lambda)))$ \textbf{in} path\_rollouts}{%
					$policy\_update \mathrel{+} = \sum_{t=0}^{T-2} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \ \mathcal{A}_t^{\psi}$\;
					$value\_update \mathrel{+} = \sum_{t=0}^{T-2} \nabla_{\psi} (R_t(\lambda) - V_{\psi}(s_t))^2$\;
				}
				Compute the entropy regularization term $H_{\pi_{\theta}}$\;
				$policy\_update = - \frac{policy\_update}{|path\_rollouts|}$, $\ value\_update = \frac{value\_update}{|path\_rollouts| \cdot (T-1)}$\;
				$\theta = \theta - lr \cdot (policy\_update + \beta H_{\pi_{\theta}})$\;
				$\psi = \psi - lr \cdot value\_update$\;
				$path\_rollouts = [\,]$\;
				$step = step + 1$\;
			}
		}
	}
\end{algorithm}
